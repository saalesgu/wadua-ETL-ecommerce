# Data Pipeline con AWS, Airflow y MotherDuck

![AWS Lambda](https://img.shields.io/badge/AWS%20Lambda-FF9900?style=for-the-badge&logo=awslambda&logoColor=white)
![API Gateway](https://img.shields.io/badge/AWS%20API%20Gateway-8C4FFF?style=for-the-badge&logo=amazonapigateway&logoColor=white)
![Amazon S3](https://img.shields.io/badge/Amazon%20S3-569A31?style=for-the-badge&logo=amazons3&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=apacheairflow&logoColor=white)
![MotherDuck](https://img.shields.io/badge/MotherDuck-000000?style=for-the-badge&logo=duckdb&logoColor=yellow)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)

Este proyecto implementa un **pipeline de datos en la nube** que procesa archivos desde **AWS S3**, aplica validaciones y transformaciones mediante **AWS Lambda**, almacena datos procesados en **MotherDuck (DuckDB en la nube)** y expone resultados a travÃ©s de una API con **API Gateway**.  
La orquestaciÃ³n del flujo estÃ¡ gestionada con **Apache Airflow** y el despliegue se realiza dentro de **Docker**.

---

## ğŸ”„ Flujo de trabajo

1. **Carga de archivos** â†’ Se suben archivos raw (ej: orders) a un bucket en **S3**.  
2. **DetecciÃ³n con Airflow** â†’ Una tarea en **Airflow** valida si hay nuevos archivos en S3.  
3. **Procesamiento en Lambda** â†’ Si hay archivos nuevos:
   - Se extraen los datos.
   - Se validan y transforman.
   - Se insertan en **MotherDuck**.
   - Se generan **logs** que se guardan en S3.  
4. **ExposiciÃ³n de resultados** â†’ Los datos procesados pueden consultarse vÃ­a:
   - **API Gateway** (integraciÃ³n con Lambda).  
5. **Notificaciones** â†’ Se envÃ­an alertas por **SNS o email** en caso de errores.

---

## Diagrama del Workflow

![alt text](wadua-flow.png)


---

## âš™ï¸ Requerimientos

- **Docker y Docker Compose** instalados.  
- **Cuenta de AWS** con permisos para:
  - S3  
  - Lambda  
  - API Gateway  
- **Credenciales AWS** configuradas (`~/.aws/credentials`).  
- Dependencias necesarias:
  - `pandas`  
  - `boto3`  
  - `duckdb`  
  - `sqlalchemy`  


## Correr este proyecto localmente

1. Clonar el repositorio:

   ```bash
   git clone https://github.com/saalesgu/wadua-ETL-ecommerce.git
   cd wadua-ETL-ecommerce
   ```

2. Levantar los servicios con Docker

   ```bash
   docker-compose up -d
   ```

3. Acceder a Airflow en `http://localhost:8080`

# ğŸ“‚ Estructura del Proyecto

```plaintext
WADUA-PROYECTO/
â”œâ”€â”€ api-wadua/                    # CÃ³digo relacionado con la API Lambda
â”‚   â”œâ”€â”€ Dockerfile                # Imagen Docker para desplegar Lambda
â”‚   â”œâ”€â”€ lambda_function.py        # FunciÃ³n principal Lambda
â”‚   â””â”€â”€ requirements.txt          # Dependencias de la Lambda
â”‚
â”œâ”€â”€ dags/                         # DefiniciÃ³n de DAGs en Airflow
â”‚   â””â”€â”€ wadua-dag/
â”‚       â”œâ”€â”€ __pycache__/          # Archivos compilados de Python
â”‚       â”œâ”€â”€ dag.py                # DefiniciÃ³n principal del DAG
â”‚       â””â”€â”€ etl.py                # LÃ³gica ETL usada en el DAG
â”‚
â”œâ”€â”€ data/                         # Espacio para almacenar datasets o archivos temporales
â”‚
â”œâ”€â”€ lambda-wadua/                 # Otro paquete Lambda (independiente)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ lambda_function.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”‚
â”œâ”€â”€ notebooks/                    # Jupyter Notebooks para anÃ¡lisis exploratorio
â”‚   â””â”€â”€ 01-EDA.ipynb              # AnÃ¡lisis de datos inicial
â”‚
â”œâ”€â”€ server/                       # Servidor backend (Flask)
â”‚   â”œâ”€â”€ static/                   # Archivos estÃ¡ticos (CSS, JS, imÃ¡genes)
â”‚   â”œâ”€â”€ templates/                # Plantillas HTML (Jinja2, etc.)
â”‚   â””â”€â”€ app.py                    # Entrada principal del servidor web
â”‚
â”œâ”€â”€ .gitignore                    # Archivos/carpetas ignoradas en git
â”œâ”€â”€ docker-compose.yml            # OrquestaciÃ³n de contenedores
â”œâ”€â”€ README.md                     # DocumentaciÃ³n principal del proyecto
â””â”€â”€ requirements.txt              # Dependencias principales del proyecto
```

---

Gracias por tomarte el tiempo de explorar este proyecto. El objetivo es seguir mejorando la arquitectura y optimizando los flujos de datos para que esta soluciÃ³n sirva como base en futuros desarrollos.  

Si tienes ideas, comentarios o sugerencias, no dudes en compartirlos.  

Â¡Toda contribuciÃ³n es bienvenida y valorada! ğŸ™Œ  